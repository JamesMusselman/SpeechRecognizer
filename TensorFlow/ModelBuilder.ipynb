{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Instantiate the random variable seed. (This wil give us a greater level of variability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.api._v2.keras as keras\n",
    "\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras import models\n",
    "# from IPython import display\n",
    "\n",
    "from train import *\n",
    "from audio import *\n",
    "from spectogram import *\n",
    "\n",
    "seed = 66\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Retrieve entries sorted by the type of audio sample and add them to \"commands\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands: ['down' 'go' 'left' 'no' 'right' 'stop' 'up' 'yes']\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = 'data/mini_speech_commands'\n",
    "data_dir = pathlib.Path(DATASET_PATH)\n",
    "\n",
    "model._commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n",
    "model._commands = model._commands[model._commands != 'README.md']\n",
    "print('Commands:', model._commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Exctract and randomized audio files into local array called \"filenames\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 8000\n",
      "Samples per category:  1000\n"
     ]
    }
   ],
   "source": [
    "filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n",
    "\n",
    "# Shuffle the files to have a random set of samples\n",
    "filenames = tf.random.shuffle(filenames)\n",
    "\n",
    "num_samples = len(filenames)\n",
    "print('Total samples:', num_samples)\n",
    "print('Samples per category: ', len(tf.io.gfile.listdir(str(data_dir/model._commands[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Sort audio files into training data, validation data, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size 6400\n",
      "Validation set size 800\n",
      "Test set size 800\n"
     ]
    }
   ],
   "source": [
    "eightyPercent = round(num_samples * 0.8)\n",
    "tenPercent = round(num_samples * 0.1)\n",
    "\n",
    "train_files = filenames[:eightyPercent]\n",
    "val_files = filenames[eightyPercent: eightyPercent + tenPercent]\n",
    "test_files = filenames[-tenPercent:]\n",
    "\n",
    "print('Training set size', len(train_files))\n",
    "print('Validation set size', len(val_files))\n",
    "print('Test set size', len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the training set to extract the audio-label pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._autotune = tf.data.AUTOTUNE\n",
    "\n",
    "# TensorSliceDataSet\n",
    "files_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "\n",
    "waveform_ds = files_ds.map(\n",
    "    map_func=get_waveform_and_label,\n",
    "    num_parallel_calls=model._autotune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test retrieving and converting a waveform into a spectrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: no\n",
      "Waveform shape: (16000,)\n",
      "Spectrogram shape: (124, 129, 1)\n"
     ]
    }
   ],
   "source": [
    "for waveform, label in waveform_ds.take(1):\n",
    "  label = label.numpy().decode('utf-8')\n",
    "  spectrogram = get_spectrogram(waveform)\n",
    "\n",
    "print('Label:', label)\n",
    "print('Waveform shape:', waveform.shape)\n",
    "print('Spectrogram shape:', spectrogram.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset with each spectrogram associated with appropriate label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram_ds = waveform_ds.map(\n",
    "    map_func=get_spectrogram_and_label_id,\n",
    "    num_parallel_calls=model._autotune\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building and training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the audio files by converting them all to Spectrogram tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = spectrogram_ds\n",
    "val_ds = preprocess_dataset(val_files)\n",
    "test_ds = preprocess_dataset(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group training and validation sets into batches of 64 items each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "val_ds = val_ds.batch(batch_size)\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(model._autotune)\n",
    "val_ds = val_ds.cache().prefetch(model._autotune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to Train the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (124, 129, 1)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resizing_2 (Resizing)       (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " normalization_3 (Normalizat  (None, 32, 32, 1)        3         \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 30, 30, 32)        320       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               1605760   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,625,611\n",
      "Trainable params: 1,625,608\n",
      "Non-trainable params: 3\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for spectrogram, _ in spectrogram_ds.take(1):\n",
    "  input_shape = spectrogram.shape\n",
    "print('Input shape:', input_shape)\n",
    "num_labels = len(model._commands)\n",
    "\n",
    "norm_layer = keras.layers.Normalization()\n",
    "norm_layer.adapt(data=spectrogram_ds.map(map_func=lambda spec, label: spec))\n",
    "\n",
    "training_model = keras.models.Sequential()\n",
    "training_model.add(keras.layers.Input(shape=input_shape))\n",
    "training_model.add(keras.layers.Resizing(32, 32))\n",
    "training_model.add(norm_layer)\n",
    "training_model.add(keras.layers.Conv2D(32, 3, activation='relu'))\n",
    "training_model.add(keras.layers.Conv2D(64, 3, activation='relu'))\n",
    "training_model.add(keras.layers.MaxPooling2D())\n",
    "training_model.add(keras.layers.Dropout(0.25))\n",
    "training_model.add(keras.layers.Flatten())\n",
    "training_model.add(keras.layers.Dense(128, activation='relu'))\n",
    "training_model.add(keras.layers.Dropout(0.5))\n",
    "training_model.add(keras.layers.Dense(num_labels))\n",
    "\n",
    "# training_model = keras.models.Sequential([\n",
    "#     keras.layers.Input(shape=input_shape),\n",
    "#     # Downsample the input.\n",
    "#     keras.layers.Resizing(32, 32),\n",
    "#     # Normalize.\n",
    "#     norm_layer,\n",
    "#     keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "#     keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "#     keras.layers.MaxPooling2D(),\n",
    "#     keras.layers.Dropout(0.25),\n",
    "#     keras.layers.Flatten(),\n",
    "#     keras.layers.Dense(128, activation='relu'),\n",
    "#     keras.layers.Dropout(0.5),\n",
    "#     keras.layers.Dense(num_labels),\n",
    "# ])\n",
    "\n",
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for training the data by specifying the loss function (Sparse Categorical Crossentropy) and the optimizer function (Adam optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 10 epochs of traning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 21s 203ms/step - loss: 1.7315 - accuracy: 0.3880 - val_loss: 1.3236 - val_accuracy: 0.5612\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 1.1620 - accuracy: 0.6023 - val_loss: 0.9703 - val_accuracy: 0.6800\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 12s 120ms/step - loss: 0.8728 - accuracy: 0.6903 - val_loss: 0.7793 - val_accuracy: 0.7375\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.6842 - accuracy: 0.7534 - val_loss: 0.7148 - val_accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.5829 - accuracy: 0.7923 - val_loss: 0.6293 - val_accuracy: 0.7788\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 0.5167 - accuracy: 0.8139 - val_loss: 0.5748 - val_accuracy: 0.7900\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 12s 120ms/step - loss: 0.4590 - accuracy: 0.8381 - val_loss: 0.5614 - val_accuracy: 0.8062\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 12s 118ms/step - loss: 0.4076 - accuracy: 0.8594 - val_loss: 0.5594 - val_accuracy: 0.8062\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 12s 117ms/step - loss: 0.3602 - accuracy: 0.8737 - val_loss: 0.5231 - val_accuracy: 0.8175\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 12s 119ms/step - loss: 0.3228 - accuracy: 0.8922 - val_loss: 0.5144 - val_accuracy: 0.8175\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "history = training_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=keras.callbacks.EarlyStopping(verbose=1, patience=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the testing data to evalue the model accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio = []\n",
    "test_labels = []\n",
    "\n",
    "for audio, label in test_ds:\n",
    "  test_audio.append(audio.numpy())\n",
    "  test_labels.append(label.numpy())\n",
    "\n",
    "test_audio = np.array(test_audio)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the tests on all the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 18ms/step\n",
      "Test set accuracy: 85%\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(training_model.predict(test_audio), axis=1)\n",
    "y_true = test_labels\n",
    "\n",
    "test_acc = sum(y_pred == y_true) / len(y_true)\n",
    "print(f'Test set accuracy: {test_acc:.0%}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84431228039547b22d69ef68bc3b261c995ed8cdf01176eaf0a3954a2e49231d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
